{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "url = \"https://newyork.craigslist.org/search/sss?query=bikes\"\n",
    "response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "if response.status_code == 200:\n",
    "    print(\"Success\")\n",
    "else:\n",
    "    print(\"Failure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_page = BeautifulSoup(response.content,'lxml')\n",
    "\n",
    "# find the total number of posts\n",
    "results_num = results_page.find('div', class_= 'search-legend')\n",
    "results_total = int(results_num.find('span', class_='totalcount').text)\n",
    "\n",
    "prices = []\n",
    "neighborhood = []\n",
    "urls = []\n",
    "dates = []\n",
    "\n",
    "pages = np.arange(0, results_total+1, 120)\n",
    "\n",
    "# Loop through all pages\n",
    "for page in pages:\n",
    "    \n",
    "    response = requests.get(url \n",
    "                   + \"s=\" #the parameter for defining the page number \n",
    "                   + str(page) #the page number in the pages array from earlier\n",
    "                   + \"&hasPic=1\"\n",
    "                   + \"&availabilityMode=0\")\n",
    "\n",
    "    sleep(0.5)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(('Request: {}; Status code: {}'.format(requests, response.status_code)))\n",
    "    \n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    posts = results_page.find_all('li', class_= 'result-row')\n",
    "\n",
    "    for post in posts:\n",
    "        prices.append(int(post.find('span', class_=\"result-price\").text[1:].replace(',', '')))\n",
    "        neighborhood.append(post.find('span', class_=\"result-hood\").text)\n",
    "        urls.append(post.find('a', class_=\"result-title hdrlnk\")['href'])\n",
    "        dates.append(post.find('time', class_=\"result-date\")['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>(Queens)</td>\n",
       "      <td>https://newyork.craigslist.org/que/spo/d/rocka...</td>\n",
       "      <td>2022-04-13 18:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>(Brooklyn Queens Manhattan Bronx)</td>\n",
       "      <td>https://newyork.craigslist.org/brk/mpo/d/astor...</td>\n",
       "      <td>2022-04-13 15:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>(Brooklyn)</td>\n",
       "      <td>https://newyork.craigslist.org/brk/bik/d/brook...</td>\n",
       "      <td>2022-04-13 13:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>(Shelton)</td>\n",
       "      <td>https://newyork.craigslist.org/fct/wan/d/bmx-b...</td>\n",
       "      <td>2022-04-13 13:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>(Ridgewood)</td>\n",
       "      <td>https://newyork.craigslist.org/brk/bik/d/ridge...</td>\n",
       "      <td>2022-04-13 11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3115</th>\n",
       "      <td>190</td>\n",
       "      <td>(BROOKLYN)</td>\n",
       "      <td>https://newyork.craigslist.org/brk/bik/d/brook...</td>\n",
       "      <td>2022-03-31 11:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3116</th>\n",
       "      <td>0</td>\n",
       "      <td>(westchester)</td>\n",
       "      <td>https://newyork.craigslist.org/wch/wan/d/maybr...</td>\n",
       "      <td>2022-03-31 09:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3117</th>\n",
       "      <td>0</td>\n",
       "      <td>(westchester)</td>\n",
       "      <td>https://newyork.craigslist.org/wch/wan/d/maybr...</td>\n",
       "      <td>2022-03-31 09:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3118</th>\n",
       "      <td>250</td>\n",
       "      <td>(West Village)</td>\n",
       "      <td>https://newyork.craigslist.org/mnh/bik/d/new-y...</td>\n",
       "      <td>2022-03-31 08:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119</th>\n",
       "      <td>620</td>\n",
       "      <td>(Brooklyn)</td>\n",
       "      <td>https://newyork.craigslist.org/brk/bik/d/brook...</td>\n",
       "      <td>2022-03-30 16:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3120 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Price                        Neighborhood  \\\n",
       "0       100                            (Queens)   \n",
       "1        75   (Brooklyn Queens Manhattan Bronx)   \n",
       "2        60                          (Brooklyn)   \n",
       "3         0                           (Shelton)   \n",
       "4       200                         (Ridgewood)   \n",
       "...     ...                                 ...   \n",
       "3115    190                          (BROOKLYN)   \n",
       "3116      0                       (westchester)   \n",
       "3117      0                       (westchester)   \n",
       "3118    250                      (West Village)   \n",
       "3119    620                          (Brooklyn)   \n",
       "\n",
       "                                                    URL              Date  \n",
       "0     https://newyork.craigslist.org/que/spo/d/rocka...  2022-04-13 18:24  \n",
       "1     https://newyork.craigslist.org/brk/mpo/d/astor...  2022-04-13 15:52  \n",
       "2     https://newyork.craigslist.org/brk/bik/d/brook...  2022-04-13 13:19  \n",
       "3     https://newyork.craigslist.org/fct/wan/d/bmx-b...  2022-04-13 13:05  \n",
       "4     https://newyork.craigslist.org/brk/bik/d/ridge...  2022-04-13 11:54  \n",
       "...                                                 ...               ...  \n",
       "3115  https://newyork.craigslist.org/brk/bik/d/brook...  2022-03-31 11:35  \n",
       "3116  https://newyork.craigslist.org/wch/wan/d/maybr...  2022-03-31 09:14  \n",
       "3117  https://newyork.craigslist.org/wch/wan/d/maybr...  2022-03-31 09:14  \n",
       "3118  https://newyork.craigslist.org/mnh/bik/d/new-y...  2022-03-31 08:58  \n",
       "3119  https://newyork.craigslist.org/brk/bik/d/brook...  2022-03-30 16:46  \n",
       "\n",
       "[3120 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Price':prices, 'Neighborhood':neighborhood, 'URL':urls, 'Date':dates})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(df['Price']!=0).dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request: <module 'requests' from 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\requests\\\\__init__.py'>; Status code: 403\n",
      "338\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'data_craigslist.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11596/2320106073.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Request: {}; Status code: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m338\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data_craigslist.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3561\u001b[0m         )\n\u001b[0;32m   3562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3563\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3564\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3565\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1178\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         )\n\u001b[1;32m-> 1180\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \"\"\"\n\u001b[0;32m    240\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'data_craigslist.csv'"
     ]
    }
   ],
   "source": [
    "for ind in df[338:].index:\n",
    "\n",
    "    url_ = df.loc[ind, 'URL']\n",
    "    \n",
    "    sleep(1)\n",
    "\n",
    "    response = requests.get(url_, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(('Request: {}; Status code: {}'.format(requests, response.status_code)))\n",
    "        print(ind)\n",
    "        df_old = pd.read_csv('data_craigslist.csv')\n",
    "        new_df = pd.concat(df_old, df[338:])\n",
    "        new_df.to_csv('data_craigslist2.csv')\n",
    "        break\n",
    "\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    all_attributes = page_html.find('p', class_= 'attrgroup')\n",
    "\n",
    "    # Finds all the attributes\n",
    "    try:\n",
    "        attr = all_attributes.find_all('span')\n",
    "\n",
    "        for att in attr:\n",
    "            str_attr = att.text.split(':')\n",
    "            attribute, value = str_attr\n",
    "            if attribute not in df.columns:\n",
    "                df[attribute] = [np.nan for _ in range(len(df))]\n",
    "            df.loc[ind, attribute] = value\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Finds the number of images\n",
    "    try:\n",
    "        user_body = page_html.find('section', class_=\"userbody\")\n",
    "        multi_img = user_body.find('figure', class_=\"iw multiimage\")\n",
    "        num_img = multi_img.find('span', class_=\"slider-info\").text.split(' ')[-1]\n",
    "        if 'num image' not in df.columns:\n",
    "            df['num image'] = [0 for _ in range(len(df))]\n",
    "        df.loc[ind, 'num image'] = num_img\n",
    "    except:\n",
    "        if 'num image' not in df.columns:\n",
    "            df['num image'] = [0 for _ in range(len(df))]\n",
    "        df.loc[ind, 'num image'] = 0\n",
    "\n",
    "    # Find the text\n",
    "    try:\n",
    "        user_text = str(page_html.find('section', {'id':\"postingbody\"}).text).strip().replace('QR Code Link to This Post', '').replace('\\n', ' ').strip()\n",
    "        if 'text' not in df.columns:\n",
    "                df['text'] = [np.nan for _ in range(len(df))]\n",
    "        df.loc[ind, 'text'] = user_text\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_craigslist.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
